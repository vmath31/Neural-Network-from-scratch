{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, epochs, learning_rate, batch_size,nodes, rho):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.rho=rho\n",
    "        self.nodes=nodes\n",
    "        self.epochs=epochs\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def fit(self, x, y, w_b, t):\n",
    "        z1, z2, z3, a1, a2, output=NeuralNetwork.feed_forward(self,x, w_b, t)\n",
    "        grads=NeuralNetwork.backprop(self, x, y, z1, z2, z3, a1, a2, output, w_b)\n",
    "        return output, grads    \n",
    "      \n",
    "    def evaluate(self, output, y):\n",
    "        accuracy = np.mean(np.argmax(y, axis=1) == np.argmax(output, axis=1))\n",
    "        return accuracy*100\n",
    "    \n",
    "    def feed_forward(self, x, w_b, t):\n",
    "        z1=np.dot(x, w_b[0]) + w_b[1]\n",
    "        a1=NeuralNetwork.ReLu(self, z1)\n",
    "        if t==0:\n",
    "            a1=NeuralNetwork.dropout(self,a1, 0.2)\n",
    "\n",
    "        z2=np.dot(a1, w_b[2]) + w_b[3]\n",
    "        a2=NeuralNetwork.ReLu(self,z2)\n",
    "        if t==0:\n",
    "            a2=NeuralNetwork.dropout(self, a2, 0.2)\n",
    "            \n",
    "        z3=np.dot(a2, w_b[4]) + w_b[5]\n",
    "        output=NeuralNetwork.softmax(self,z3)        \n",
    "        \n",
    "#    print(\"z1\", z1.shape)\n",
    "#    print(\"a1\", a1.shape)\n",
    "#    print(\"w1\", w_b[0].shape)\n",
    "#    print(\"b1\", w_b[1].shape)\n",
    "#    print(\"z2\", z2.shape)\n",
    "#    print(\"a2\", a2.shape)\n",
    "#    print(\"w2\", w_b[2].shape)\n",
    "#    print(\"b2\", w_b[3].shape)\n",
    "#    print(\"z3\", z3.shape)\n",
    "#    print(\"output\", output.shape)\n",
    "#    print(\"w3\", w_b[4].shape)\n",
    "#    print(\"b3\", w_b[5].shape)\n",
    "        if t==1:\n",
    "            return output\n",
    "        else:\n",
    "            return z1, z2, z3, a1, a2, output\n",
    "  \n",
    "    def backprop(self, x, y, z1, z2, z3, a1, a2, output, w_b):\n",
    "        \n",
    "        #https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7\n",
    "        #memoization\n",
    "        a3_delta = output - y\n",
    "        z2_delta = np.dot(a3_delta, w_b[4].T)\n",
    "        a2_delta = z2_delta * NeuralNetwork.derv_relu(self,z2) \n",
    "        z1_delta = np.dot(a2_delta, w_b[2].T)\n",
    "        a1_delta = z1_delta * NeuralNetwork.derv_relu(self, z1) \n",
    "        \n",
    "        #gradient calculation\n",
    "        L_dw3 = np.dot(a2.T, a3_delta)\n",
    "        L_db3 = np.sum(a3_delta, axis=0).reshape(1,-1)\n",
    "        L_dw2 = np.dot(a1.T, a2_delta)\n",
    "        L_db2 = np.sum(a2_delta, axis=0).reshape(1,-1)\n",
    "        L_dw1 = np.dot(x.T, a1_delta)\n",
    "        L_db1 = np.sum(a1_delta, axis=0).reshape(1,-1)\n",
    "\n",
    "        return [L_dw1, L_db1, L_dw2, L_db2,  L_dw3, L_db3]            \n",
    "\n",
    "\n",
    "    def ReLu(self,layer):\n",
    "        return np.maximum(0, layer)\n",
    "\n",
    "#https://gluon.mxnet.io/chapter03_deep-neural-networks/mlp-dropout-scratch.html\n",
    "    def dropout(self, x, drop_probability):\n",
    "        keep_probability = 1 - drop_probability\n",
    "        mask = np.random.uniform(0, 1.0, x.shape) < keep_probability\n",
    "        if keep_probability > 0.0:\n",
    "            scale = (1/keep_probability)\n",
    "        else:\n",
    "            scale = 0.0\n",
    "        return mask * x * scale\n",
    "\n",
    "    def softmax(self, layer):\n",
    "        exp = np.exp(layer-np.max(layer, axis=1).reshape(-1,1))\n",
    "        sum_e = np.sum(exp, axis=1).reshape(-1,1)\n",
    "        return exp / sum_e\n",
    "#        for i in range(layer.shape[0]):\n",
    "#            e_x=np.exp(layer[i,:]-np.max(layer[i,:]))\n",
    "#            s=np.sum(e_x)\n",
    "#            layer[i,:]= e_x/s\n",
    "#        return layer\n",
    "\n",
    "    def derv_softmax(self, layer):\n",
    "        return np.dot(layer.T,(1-layer))\n",
    "\n",
    "    def derv_relu(self, layer):\n",
    "        layer[layer > 0]=1\n",
    "        layer[layer <= 0]=0\n",
    "        return layer\n",
    "    \n",
    "#https://gluon.mxnet.io/chapter06_optimization/rmsprop-scratch.html\n",
    "    def RMSProp(self, w_b, S, grads, rho, lr):\n",
    "        for wb, grad, s in zip(w_b, grads, S):\n",
    "            s=rho*s + (1-rho)*np.square(grad) \n",
    "            d=lr*grad/np.sqrt(s+1e-8)\n",
    "            wb-=d \n",
    "        return w_b\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [14000 14001 14002 ... 69997 69998 69999] TEST: [    0     1     2 ... 13997 13998 13999]\n",
      "FOLD:  1\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Train Accuracy: 73.99464285714285\n",
      "Test Accuracy: 74.12857142857143\n",
      "TRAIN: [    0     1     2 ... 69997 69998 69999] TEST: [14000 14001 14002 ... 27997 27998 27999]\n",
      "FOLD:  2\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Train Accuracy: 73.89642857142857\n",
      "Test Accuracy: 73.7\n",
      "TRAIN: [    0     1     2 ... 69997 69998 69999] TEST: [28000 28001 28002 ... 41997 41998 41999]\n",
      "FOLD:  3\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Train Accuracy: 74.2625\n",
      "Test Accuracy: 74.25\n",
      "TRAIN: [    0     1     2 ... 69997 69998 69999] TEST: [42000 42001 42002 ... 55997 55998 55999]\n",
      "FOLD:  4\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Train Accuracy: 76.5625\n",
      "Test Accuracy: 76.5\n",
      "TRAIN: [    0     1     2 ... 55997 55998 55999] TEST: [56000 56001 56002 ... 69997 69998 69999]\n",
      "FOLD:  5\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data_x= np.concatenate((x_train, x_test))\n",
    "data_y= np.concatenate((y_train, y_test))\n",
    "\n",
    "nodes=512\n",
    "epochs=20\n",
    "rho=0.9\n",
    "lr=0.001\n",
    "batch_size=128\n",
    "\n",
    "model=NeuralNetwork(epochs=epochs, learning_rate=lr, batch_size=batch_size,nodes=nodes, rho=rho)\n",
    "\n",
    "#https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "#w1=np.random.randn(x_train.shape[1], nodes) * np.sqrt(2/(x_train.shape[1]+nodes))\n",
    "#w2=np.random.randn(nodes, nodes) * np.sqrt(2/(nodes+nodes))\n",
    "#w3=np.random.randn(nodes, y_train.shape[1]) * np.sqrt(2/(nodes+y_train.shape[1]))\n",
    "#tried out different intialisations\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "count=1\n",
    "for train_index, test_index in kfold.split(data_x):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    x_train, x_test = data_x[train_index], data_x[test_index]\n",
    "    y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "\n",
    "    print(\"FOLD: \", count)\n",
    "    count+=1\n",
    "\n",
    "    x=x_train\n",
    "    y=y_train\n",
    "    \n",
    "    w1=np.random.normal(0, 2/(x_train.shape[1]+nodes), (x_train.shape[1], nodes))\n",
    "    w2=np.random.normal(0, 2/(nodes+nodes), (nodes, nodes))\n",
    "    w3=np.random.normal(0, 2/(nodes+y.shape[1]), (nodes,y_train.shape[1]))\n",
    "\n",
    "    b1=np.zeros((1, nodes))\n",
    "    b2=np.zeros((1, nodes))\n",
    "    b3=np.zeros((1, y_train.shape[1]))\n",
    "    w_b=[w1,b1,w2,b2,w3,b3]\n",
    "    S=[0,0,0,0,0,0]\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        iter=int(x.shape[0]/batch_size)\n",
    "        print(\"Epoch\", i+1)\n",
    "        l=[]\n",
    "\n",
    "        for j in range(iter):\n",
    "            output, grads = model.fit(x[j*batch_size:(j+1)*batch_size, :], y[j*batch_size:(j+1)*batch_size, :], w_b, 0)\n",
    "            l.append(np.sum(-y[j*batch_size:(j+1)*batch_size, :]*np.log(output+1e-12)/batch_size))\n",
    "            #grads = model.backprop(x[j*batch_size:(j+1)*batch_size, :], y[j*batch_size:(j+1)*batch_size, :], z1, z2, z3, a1, a2, output, w_b)\n",
    "            w_b=model.RMSProp(w_b, S, grads ,rho, lr)\n",
    "            \n",
    "            if j==iter-1 and x.shape[0]/batch_size!=0:\n",
    "                output, grads = model.fit(x[(j+1)*batch_size:, :], y[(j+1)*batch_size:, :], w_b, 0)\n",
    "                l.append(np.sum(-y[(j+1)*batch_size:, :]*np.log(output+1e-12)/y[(j+1)*batch_size:, :].shape[0]))\n",
    "                #grads = model.backprop(x[j*batch_size:(j+1)*batch_size, :], y[j*batch_size:(j+1)*batch_size, :], z1, z2, z3, a1, a2, output, w_b)\n",
    "                w_b=model.RMSProp(w_b, S, grads ,rho, lr)\n",
    "                \n",
    "\n",
    "    output_train=model.feed_forward(x, w_b, 1)\n",
    "    b=model.evaluate(output_train, y)\n",
    "    output_test=model.feed_forward(x_test, w_b, 1)\n",
    "    a=model.evaluate(output_test, y_test)    \n",
    "    print(\"Train Accuracy:\", b)   \n",
    "    print(\"Test Accuracy:\", a) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
